{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7rZnJaGTWQw0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "#mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "#mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": "['./data/preliminary_testing_walking_18_0_2022_12_12-20_19_01.csv',\n './data/preliminary_testing_walking_18_15_2022_12_12-20_24_50.csv',\n './data/preliminary_testing_walking_08_15_2022_12_12-20_22_39.csv',\n './data/preliminary_testing_walking_17_0_2022_12_12-20_18_30.csv',\n './data/preliminary_testing_walking_15_0_2022_12_12-20_16_35.csv',\n './data/preliminary_testing_walking_15_15_2022_12_12-20_24_17.csv',\n './data/preliminary_testing_walking_16_0_2022_12_12-20_20_55.csv',\n './data/preliminary_testing_walking_07_15_2022_12_12-20_25_23.csv',\n './data/preliminary_testing_walking_11_0_2022_12_12-20_20_26.csv',\n './data/preliminary_testing_stairup_0_0_2022_12_12-20_34_19.csv',\n './data/preliminary_testing_walking_05_15_2022_12_12-20_22_04.csv',\n './data/preliminary_testing_walking_08_0_2022_12_12-20_15_29.csv',\n './data/preliminary_testing_walking_10_0_2022_12_12-20_16_02.csv',\n './data/preliminary_testing_walking_10_15_2022_12_12-20_23_18.csv',\n './data/preliminary_testing_walking_13_15_2022_12_12-20_23_49.csv',\n './data/preliminary_testing_walking_13_0_2022_12_12-20_19_30.csv',\n './data/preliminary_testing_walking_09_0_2022_12_12-20_19_58.csv',\n './data/preliminary_testing_stairdown_0_0_2022_12_12-20_34_54.csv',\n './data/preliminary_testing_stairup_2_0_2022_12_12-20_51_35.csv',\n './data/preliminary_testing_stairdown_2_0_2022_12_12-20_52_08.csv',\n './data/preliminary_testing_walking_12_0_2022_12_12-20_17_31.csv',\n './data/preliminary_testing_walking_13_0_2022_12_12-20_18_01.csv']"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading/Creating the paths to Figshare dataset\n",
    "path = r'./data/'\n",
    "#path = r'/home/ubuntu/MIE1075/data/'\n",
    "dir = []\n",
    "for filename in os.listdir(path):\n",
    "\n",
    "    if os.path.isfile(os.path.join(path, filename)):\n",
    "        if 'walking_' in filename or '_stairup_' in filename or '_stairdown' in filename:\n",
    "            dir.append(os.path.join(path, filename))\n",
    "print(len(dir))\n",
    "dir"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "columns_tofilter = ['position_1','position_2','position_3',\n",
    " 'position_4',\n",
    " 'velocity_1',\n",
    " 'velocity_2',\n",
    " 'velocity_3',\n",
    " 'velocity_4',\n",
    " 'att_w',\n",
    " 'att_x',\n",
    " 'att_y',\n",
    " 'att_z',\n",
    " 'rate_dps_x',\n",
    " 'rate_dps_y',\n",
    " 'rate_dps_z',\n",
    " 'accel_mps2_x',\n",
    " 'accel_mps2_y',\n",
    " 'accel_mps2_z',\n",
    " 'euler_rad_x',\n",
    " 'euler_rad_y',\n",
    " 'euler_rad_z']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# function for butterworth '\n",
    "def butter_filt(signal_data):\n",
    "    fs = 38\n",
    "    fc = 8  # Cut-off frequency of the filter\n",
    "    w = fc / (fs / 2) # Normalize the frequency\n",
    "    b, a = signal.butter(6, w/2, 'lowpass')\n",
    "    return signal.filtfilt(b, a, signal_data)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def df_to_datapoints(df, num_input=40, prediction_offset=1, output_column=None, filter=True):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    df = df.drop(['timestamp'], axis=1)\n",
    "    if filter == True:\n",
    "        df[columns_tofilter] = df[columns_tofilter].apply(butter_filt)\n",
    "    for i in range(len(df) - num_input - prediction_offset):\n",
    "        x = df[i:i+num_input]\n",
    "        y = df.iloc[[i + num_input + prediction_offset]]\n",
    "\n",
    "        if output_column is not None:\n",
    "            y = y[output_column]\n",
    "\n",
    "        x_list.append(x.values)\n",
    "        y_list.append(y.values)\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "#sample_df = pd.read_csv('/home/ubuntu/MIE1075/data/preliminary_testing_normalwalk_0_0_2022_12_12-20_53_28.csv')\n",
    "#df_to_datapoints(sample_df, output_column=['position_1', 'velocity_1','position_3', 'velocity_3'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/preliminary_testing_walking_18_0_2022_12_12-20_19_01.csv\n",
      "./data/preliminary_testing_walking_17_0_2022_12_12-20_18_30.csv\n",
      "./data/preliminary_testing_walking_15_0_2022_12_12-20_16_35.csv\n",
      "./data/preliminary_testing_walking_16_0_2022_12_12-20_20_55.csv\n",
      "./data/preliminary_testing_walking_11_0_2022_12_12-20_20_26.csv\n",
      "./data/preliminary_testing_walking_08_0_2022_12_12-20_15_29.csv\n",
      "./data/preliminary_testing_walking_10_0_2022_12_12-20_16_02.csv\n",
      "./data/preliminary_testing_walking_13_0_2022_12_12-20_19_30.csv\n",
      "./data/preliminary_testing_walking_09_0_2022_12_12-20_19_58.csv\n",
      "./data/preliminary_testing_walking_12_0_2022_12_12-20_17_31.csv\n",
      "./data/preliminary_testing_walking_13_0_2022_12_12-20_18_01.csv\n",
      "12769 12769 3200 3200\n",
      "(40, 22) (1, 4) (40, 22) (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# intentions are 1 = LEVEL GROUND WALKING | 2 = RAMP UP | 3 = STAIRS UP | 4 = STAIRS DOWN\n",
    "# Reading the dataset\n",
    "\n",
    "\n",
    "x_train_all = []\n",
    "y_train_all = []\n",
    "x_val_all = []\n",
    "y_val_all = []\n",
    "'''\n",
    "x_walk = []\n",
    "y_walk = []\n",
    "x_ramp = []\n",
    "y_ramp = []\n",
    "x_stairup = []\n",
    "y_stairup = []\n",
    "x_stairdown = []\n",
    "y_stairdown = []\n",
    "'''\n",
    "for f in dir:\n",
    "\n",
    "\n",
    "    if 'walking_' in f and '_0_' in f:\n",
    "        print(f)\n",
    "        temp = pd.read_csv(f, index_col=None, header=0)\n",
    "        temp['intent'] = 1\n",
    "\n",
    "    '''\n",
    "    elif 'walking_' in f and '_15_2022' in f:\n",
    "        temp['intent'] = 2\n",
    "\n",
    "    elif 'stairup' in f:\n",
    "        temp['intent'] = 3\n",
    "\n",
    "    elif 'stairdown' in f:\n",
    "        temp['intent'] = 4\n",
    "\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "    '''\n",
    "    x, y = df_to_datapoints(temp, num_input=40, prediction_offset=1, output_column=['position_1', 'velocity_1','position_3', 'velocity_3'])\n",
    "    n = len(x)\n",
    "    x_train = x[0:int(n*0.8)]\n",
    "    y_train = y[0:int(n*0.8)]\n",
    "    x_val = x[int(n*0.8):]\n",
    "    y_val = y[int(n*0.8):]\n",
    "    x_train_all += x_train\n",
    "    y_train_all += y_train\n",
    "    x_val_all += x_val\n",
    "    y_val_all += y_val\n",
    "\n",
    "    # https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "\n",
    "    #df_list.append(temp)\n",
    "#df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "#df.to_csv(path+'merged_recorded_data.csv')\n",
    "print(len(x_train_all),len(y_train_all),len(x_val_all),len(y_val_all))\n",
    "print(x_train_all[0].shape, y_train_all[0].shape,x_val_all[0].shape, y_val_all[0].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.shape(x_train_all), np.shape(y_train_all))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.shape(x_val_all), np.shape(y_val_all))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.shape(x_train_all[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train_all[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# write a for loop to do over all the data\n",
    "scalar = StandardScaler()\n",
    "x_train_all_std = []\n",
    "for i in x_train_all:\n",
    "    temp = scalar.fit_transform(i)\n",
    "    x_train_all_std.append(temp)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": "12769"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(x_train_all_std[0]))\n",
    "len(x_train_all_std)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "x_val_all_std = []\n",
    "for i in x_val_all:\n",
    "    temp = scalar.transform(i)\n",
    "    x_val_all_std.append(temp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRzj1inMfgcO"
   },
   "source": [
    "Here is the evolution of a few features over time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXWLG0_WBhZS"
   },
   "source": [
    "### Inspect and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhmZXJew6GlS"
   },
   "source": [
    "Next, look at the statistics of the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eFckdUUHWmT"
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling: subtract the mean and divide by the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train_all_std, y_train_all))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val_all_std, y_val_all))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TensorSliceDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_30894/3623739689.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m: 'TensorSliceDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([32, 1, 4])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_data.take(1):\n",
    "   pass\n",
    "\n",
    "label_batch.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "train_data = train_data.shuffle(24).batch(32)\n",
    "val_data = val_data.shuffle(24).batch(16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVJgblsYzL1g"
   },
   "source": [
    "Here is code to create the 2 windows shown in the diagrams at the start of this section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4aOJScj52Yu"
   },
   "source": [
    "## BUILD MODELS AND EVALUATE THEM\n",
    "### Linear model\n",
    "\n",
    "\n",
    "A `tf.keras.layers.Dense` layer with no `activation` set is a linear model. The layer only transforms the last axis of the data from `(batch, time, inputs)` to `(batch, time, units)`; it is applied independently to every item across the `batch` and `time` axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, train_data, val_data, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(train_data, epochs=MAX_EPOCHS, batch_size=16,\n",
    "                      validation_data=val_data, shuffle=True,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# multi output\n",
    "linear_multi = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=4)])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "9agbz2qB9bLS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7a255ffd-6650-404f-a8d4-ce89c8acc5a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 1s 1ms/step - loss: 6.8905 - mean_absolute_error: 1.6023 - val_loss: 10.1027 - val_mean_absolute_error: 2.2750\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.4232 - mean_absolute_error: 1.3893 - val_loss: 7.9068 - val_mean_absolute_error: 1.8826\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3732 - mean_absolute_error: 1.3770 - val_loss: 6.9455 - val_mean_absolute_error: 1.6456\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3617 - mean_absolute_error: 1.3811 - val_loss: 6.6128 - val_mean_absolute_error: 1.5310\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3578 - mean_absolute_error: 1.3818 - val_loss: 6.5049 - val_mean_absolute_error: 1.4775\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3557 - mean_absolute_error: 1.3810 - val_loss: 6.4635 - val_mean_absolute_error: 1.4493\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3543 - mean_absolute_error: 1.3806 - val_loss: 6.4415 - val_mean_absolute_error: 1.4306\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3533 - mean_absolute_error: 1.3801 - val_loss: 6.4247 - val_mean_absolute_error: 1.4169\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3527 - mean_absolute_error: 1.3798 - val_loss: 6.4107 - val_mean_absolute_error: 1.4068\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3521 - mean_absolute_error: 1.3790 - val_loss: 6.4032 - val_mean_absolute_error: 1.3995\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3518 - mean_absolute_error: 1.3792 - val_loss: 6.3970 - val_mean_absolute_error: 1.3950\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3516 - mean_absolute_error: 1.3786 - val_loss: 6.3945 - val_mean_absolute_error: 1.3920\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3514 - mean_absolute_error: 1.3788 - val_loss: 6.3936 - val_mean_absolute_error: 1.3909\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3513 - mean_absolute_error: 1.3787 - val_loss: 6.3924 - val_mean_absolute_error: 1.3899\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3512 - mean_absolute_error: 1.3783 - val_loss: 6.3920 - val_mean_absolute_error: 1.3892\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3511 - mean_absolute_error: 1.3783 - val_loss: 6.3917 - val_mean_absolute_error: 1.3888\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3511 - mean_absolute_error: 1.3788 - val_loss: 6.3913 - val_mean_absolute_error: 1.3885\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3511 - mean_absolute_error: 1.3789 - val_loss: 6.3910 - val_mean_absolute_error: 1.3879\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3511 - mean_absolute_error: 1.3789 - val_loss: 6.3905 - val_mean_absolute_error: 1.3875\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 6.3511 - mean_absolute_error: 1.3791 - val_loss: 6.3902 - val_mean_absolute_error: 1.3873\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(linear_multi, train_data, val_data)\n",
    "#performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 712us/step - loss: 6.3902 - mean_absolute_error: 1.3873\n"
     ]
    }
   ],
   "source": [
    "val_performance = {}\n",
    "val_performance['Linear'] = linear_multi.evaluate(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "d4uCTbsmK8VI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "outputId": "0bf77536-7800-44be-d702-0e3b5935feb2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_30894/1465679411.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m plt.bar(x = range(len(train_data.columns)),\n\u001B[0m\u001B[1;32m      2\u001B[0m         height=linear_multi.layers[0].kernel[:,0].numpy())\n\u001B[1;32m      3\u001B[0m \u001B[0maxis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgca\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xticks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xticklabels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrotation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m90\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BatchDataset' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "plt.bar(x = range(len(train_data.columns)),\n",
    "        height=linear_multi.layers[0].kernel[:,0].numpy())\n",
    "axis = plt.gca()\n",
    "axis.set_xticks(range(len(train_data.columns)))\n",
    "_ = axis.set_xticklabels(train_data.columns, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W18e6da1cNbw"
   },
   "source": [
    "### Dense\n",
    "\n",
    "Before applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.\n",
    "\n",
    "Here's a model similar to the `linear` model, except it stacks several a few `Dense` layers between the input and the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 1s 1ms/step - loss: 5.7147 - mean_absolute_error: 1.3342 - val_loss: 9.8602 - val_mean_absolute_error: 1.9973\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 4.6151 - mean_absolute_error: 1.2363 - val_loss: 15.9108 - val_mean_absolute_error: 2.5166\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 1s 1ms/step - loss: 4.1008 - mean_absolute_error: 1.1936 - val_loss: 19.4532 - val_mean_absolute_error: 2.7691\n",
      "200/200 [==============================] - 0s 670us/step - loss: 19.4532 - mean_absolute_error: 2.7691\n"
     ]
    }
   ],
   "source": [
    "# MULTI OUTPUT\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=4)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(dense, train_data, val_data)\n",
    "\n",
    "val_performance['Dense'] = dense.evaluate(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5dv_whJdswH"
   },
   "source": [
    "### Multi-step dense\n",
    "\n",
    "A single-time-step model has no context for the current values of its inputs. It can't see how the input features are changing over time. To address this issue the model needs access to multiple time steps when making predictions:\n",
    "\n",
    "![Three time steps are used for each prediction.](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/conv_window.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We0HdMxKeqB_"
   },
   "source": [
    "You could train a `dense` model on a multiple-input-step window by adding a `tf.keras.layers.Flatten` as the first layer of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "multi_step_dense_multioutput = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=4),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 611us/step - loss: 12.9156 - mean_absolute_error: 2.6330\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(multi_step_dense_multioutput, train_data,val_data)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Multi step dense'] = multi_step_dense_multioutput.evaluate(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWfrsP8mq8lV"
   },
   "source": [
    "The main down-side of this approach is that the resulting model can only be executed on input windows of exactly this shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrpU6gwSJome"
   },
   "source": [
    "### Convolution neural network\n",
    " \n",
    "A convolution layer (`tf.keras.layers.Conv1D`) also takes multiple time steps as input to each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdLBwoaHmsWb"
   },
   "source": [
    "Below is the **same** model as `multi_step_dense`, re-written with a convolution. \n",
    "\n",
    "Note the changes:\n",
    "* The `tf.keras.layers.Flatten` and the first `tf.keras.layers.Dense` are replaced by a `tf.keras.layers.Conv1D`.\n",
    "* The `tf.keras.layers.Reshape` is no longer necessary since the convolution keeps the time axis in its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "CONV_WIDTH = 8\n",
    "conv_model_multi = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32,\n",
    "                           kernel_size=(CONV_WIDTH,),\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=4),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftaH6B5ECRiK"
   },
   "source": [
    "Run it on an example batch to check that the model produces outputs with the expected shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 746us/step - loss: 73.3325 - mean_absolute_error: 5.0445\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(conv_model_multi, train_data,val_data)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Conv'] = conv_model_multi.evaluate(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYRipDeXs0Kr"
   },
   "source": [
    "The difference between this `conv_model` and the `multi_step_dense` model is that the `conv_model` can be run on inputs of any length. The convolutional layer is applied to a sliding window of inputs:\n",
    "\n",
    "![Executing a convolutional model on a sequence](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/wide_conv_window.png?raw=1)\n",
    "\n",
    "If you run it on wider input, it produces wider output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_WGxtLIHhRF"
   },
   "source": [
    "Note that the output is shorter than the input. To make training or plotting work, you need the labels, and prediction to have the same length. So build a `WindowGenerator` to produce wide windows with a few extra input time steps so the label and prediction lengths match:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4crpOcoMlSe"
   },
   "source": [
    "### Recurrent neural network\n",
    "\n",
    "A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state from time-step to time-step.\n",
    "\n",
    "You can learn more in the [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation) tutorial and the [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn) guide.\n",
    "\n",
    "In this tutorial, you will use an RNN layer called Long Short-Term Memory (`tf.keras.layers.LSTM`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfQbHSMb1ATa"
   },
   "source": [
    "An important constructor argument for all Keras RNN layers, such as `tf.keras.layers.LSTM`, is the `return_sequences` argument. This setting can configure the layer in one of two ways:\n",
    "\n",
    "1. If `False`, the default, the layer only returns the output of the final time step, giving the model time to warm up its internal state before making a single prediction: \n",
    "\n",
    "![An LSTM warming up and making a single prediction](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/lstm_1_window.png?raw=1)\n",
    "\n",
    "2. If `True`, the layer returns an output for each input. This is useful for:\n",
    "  * Stacking RNN layers. \n",
    "  * Training a model on multiple time steps simultaneously.\n",
    "\n",
    "![An LSTM making a prediction after every time step](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/lstm_many_window.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lstm_model_multi = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=4)\n",
    "])"
   ],
   "metadata": {
    "id": "slObuiMqXB0i"
   },
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F124B00KZcLC"
   },
   "source": [
    "With `return_sequences=True`, the model can be trained on 24 hours of data at a time.\n",
    "\n",
    "Note: This will give a pessimistic view of the model's performance. On the first time step, the model has no access to previous steps and, therefore, can't do any better than the simple `linear` and `dense` models shown earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "uvdWRl1e9WJl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4802da64-faf8-4cca-cee1-d8316a23b36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step - loss: 8.0633 - mean_absolute_error: 1.7540\n"
     ]
    }
   ],
   "source": [
    "history = compile_and_fit(lstm_model_multi, train_data,val_data)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['LSTM'] = lstm_model_multi.evaluate(val_data)\n",
    "#performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYglOCKehi8F"
   },
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pCk0_rwhi8H"
   },
   "source": [
    "With this dataset typically each of the models does slightly better than the one before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "JjEkt488hi8I",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "outputId": "f2dc933a-d095-4e7b-ecd9-fdc8be4b166a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(val_performance))\n",
    "width = 0.5\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = lstm_model_multi.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in performance.values()]\n",
    "\n",
    "plt.ylabel('mean_absolute_error position_1, normalized]')\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "#plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=val_performance.keys(), rotation=45)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "cBMCpsdphi8L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c1fb9b2-2902-4b84-fd0f-6cc097410fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear      : 1.3873\n",
      "Dense       : 2.7691\n",
      "Multi step dense: 2.6330\n",
      "Conv        : 5.0445\n",
      "LSTM        : 1.7540\n"
     ]
    }
   ],
   "source": [
    "for name, value in val_performance.items():\n",
    "  print(f'{name:12s}: {value[1]:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5rUJ_2YMWzG"
   },
   "source": [
    "### Multi-output models\n",
    "\n",
    "The models so far all predicted a single output feature, `T (degC)`, for a single time step.\n",
    "\n",
    "All of these models can be converted to predict multiple features just by changing the number of units in the output layer and adjusting the training windows to include all features in the `labels` (`example_labels`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Gk0Z91xjOwv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5d84a61-7ac2-4d1b-e265-ac405c9d5d70"
   },
   "outputs": [],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    # `WindowGenerator` returns all features as labels if you \n",
    "    # don't set the `label_columns` argument.\n",
    "    input_width=1, label_width=1, shift=1 ,label_columns=['position_1','position_3','velocity_1','velocity_3']\n",
    "    )\n",
    "\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=8, label_width=8, shift=1 ,label_columns=['position_1','position_3','velocity_1','velocity_3']\n",
    "    )\n",
    "\n",
    "for example_inputs, example_labels in wide_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for example_inputs, example_labels in wide_window.val.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T19fz7sJTgvC",
    "outputId": "967d9c64-c2fe-4c9d-8588-3f0f21fdec5d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for example_inputs, example_labels in single_step_window.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ICTREQbUArw",
    "outputId": "2819b545-8380-48f8-9423-993964e4be92"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmcjHfDskX1N"
   },
   "source": [
    "Note above that the `features` axis of the labels now has the same depth as the inputs, instead of `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k7S5IHNhSNF"
   },
   "source": [
    "#### Baseline\n",
    "\n",
    "The same baseline model (`Baseline`) can be used here, but this time repeating all features instead of selecting a specific `label_index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqqB9W-pjr5i"
   },
   "outputs": [],
   "source": [
    "baseline = Baseline()\n",
    "baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.keras.metrics.MeanAbsoluteError()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_performance = {}\n",
    "#performance = {}\n",
    "val_performance['Baseline'] = baseline.evaluate(wide_window.val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### trying to build the multioutput single step lstm's baseline here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    # `WindowGenerator` returns all features as labels if you\n",
    "    # don't set the `label_columns` argument.\n",
    "    input_width=1, label_width=1, shift=1 ,label_columns=['position_1','position_3','velocity_1','velocity_3']\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfbCrf5q3P6n"
   },
   "source": [
    "#### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdpzH1dYjdIN"
   },
   "outputs": [],
   "source": [
    "#TESTING THE 4 OUTPUT FUNCTION\n",
    "FEATURES_NUM = 23\n",
    "OUTPUT_NUM = 4\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=OUTPUT_NUM),\n",
    "    tf.keras.layers.Reshape([1,-1]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = compile_and_fit(dense, single_step_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Dense'] = dense.evaluate(single_step_window.val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsc9pur_mHsx"
   },
   "source": [
    "#### RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QbGLMyomXaz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "outputId": "7bc42b96-5497-4c77-be7d-342f4689e45f"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_features = 23\n",
    "OUTPUT_NUM = 4\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=8, label_width=8, shift=1#, label_columns=['position_1','position_3','velocity_1','velocity_3']\n",
    "    )\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=num_features)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\n",
    "#performance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "num_features = 23\n",
    "OUTPUT_NUM = 4\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=8, label_width=8, shift=1, label_columns=['position_1','position_3','velocity_1','velocity_3']\n",
    "    )\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=OUTPUT_NUM)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\n",
    "#performance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n",
    "\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwhY2f_Nn0_K"
   },
   "source": [
    "<a id=\"residual\"></a>\n",
    "\n",
    "#### Advanced: Residual connections\n",
    "\n",
    "The `Baseline` model from earlier took advantage of the fact that the sequence doesn't change drastically from time step to time step. Every model trained in this tutorial so far was randomly initialized, and then had to learn that the output is a a small change from the previous time step.\n",
    "\n",
    "While you can get around this issue with careful initialization, it's  simpler to build this into the model structure.\n",
    "\n",
    "It's common in time series analysis to build models that instead of predicting the next value, predict how the value will change in the next time step. Similarly, <a href=\"https://arxiv.org/abs/1512.03385\" class=\"external\">residual networks</a>or ResNetsin deep learning refer to architectures where each layer adds to the model's accumulating result.\n",
    "\n",
    "That is how you take advantage of the knowledge that the change should be small.\n",
    "\n",
    "![A model with a residual connection](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/residual.png?raw=1)\n",
    "\n",
    "Essentially, this initializes the model to match the `Baseline`. For this task it helps models converge faster, with slightly better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP58A_ORx0kM"
   },
   "source": [
    "This approach can be used in conjunction with any model discussed in this tutorial. \n",
    "\n",
    "Here, it is being applied to the LSTM model, note the use of the `tf.initializers.zeros` to ensure that the initial predicted changes are small, and don't overpower the residual connection. There are no symmetry-breaking concerns for the gradients here, since the `zeros` are only used on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YlfnDQC22TQ"
   },
   "outputs": [],
   "source": [
    "class ResidualWrapper(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "\n",
    "  def call(self, inputs, *args, **kwargs):\n",
    "    delta = self.model(inputs, *args, **kwargs)\n",
    "\n",
    "    # The prediction for each time step is the input\n",
    "    # from the previous time step plus the delta\n",
    "    # calculated by the model.\n",
    "    return inputs + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNeH02pspc9B"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "residual_lstm = ResidualWrapper(\n",
    "    tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        num_features,\n",
    "        # The predicted deltas should start small.\n",
    "        # Therefore, initialize the output layer with zeros.\n",
    "        kernel_initializer=tf.initializers.zeros()),\n",
    "\n",
    "]))\n",
    "\n",
    "history = compile_and_fit(residual_lstm, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)\n",
    "#performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "residual_lstm = ResidualWrapper(\n",
    "    tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        num_features,\n",
    "        # The predicted deltas should start small.\n",
    "        # Therefore, initialize the output layer with zeros.\n",
    "        kernel_initializer=tf.initializers.zeros()),\n",
    "\n",
    "]))\n",
    "\n",
    "history = compile_and_fit(residual_lstm, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)\n",
    "#performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I42Er9Du6co1"
   },
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZxR38P_6pUi"
   },
   "source": [
    "Here is the overall performance for these multi-output models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XgTK9tnr7rc"
   },
   "outputs": [],
   "source": [
    "x = np.arange(len(val_performance))\n",
    "width = 0.3\n",
    "\n",
    "metric_name = 'mean_absolute_error'\n",
    "metric_index = lstm_model.metrics_names.index('mean_absolute_error')\n",
    "val_mae = [v[metric_index] for v in val_performance.values()]\n",
    "#test_mae = [v[metric_index] for v in performance.values()]\n",
    "\n",
    "plt.bar(x - 0.17, val_mae, width, label='Validation')\n",
    "#plt.bar(x + 0.17, test_mae, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=val_performance.keys(),\n",
    "           rotation=45)\n",
    "plt.ylabel('MAE (average over all outputs)')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "URz3ajCc6kBj"
   },
   "outputs": [],
   "source": [
    "for name, value in val_performance.items():\n",
    "  print(f'{name:15s}: {value[1]:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col='position_1', max_subplots=3):\n",
    "  inputs, labels = self.example\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  plot_col_index = self.column_indices[plot_col]\n",
    "  max_n = min(max_subplots, len(inputs))\n",
    "  for n in range(max_n):\n",
    "    plt.subplot(max_n, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "      label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "      continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "      predictions = model(inputs)\n",
    "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "      plt.legend()\n",
    "\n",
    "  plt.xlabel('Time')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vt2MJhNxwPU"
   },
   "source": [
    "The above performances are averaged across all model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYokb7Om2YbK"
   },
   "source": [
    "## Multi-step models\n",
    "\n",
    "Both the single-output and multiple-output models in the previous sections made **single time step predictions**, one hour into the future.\n",
    "\n",
    "This section looks at how to expand these models to make **multiple time step predictions**.\n",
    "\n",
    "In a multi-step prediction, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predicts a sequence of the future values.\n",
    "\n",
    "There are two rough approaches to this:\n",
    "\n",
    "1. Single shot predictions where the entire time series is predicted at once.\n",
    "2. Autoregressive predictions where the model only makes single step predictions and its output is fed back as its input.\n",
    "\n",
    "In this section all the models will predict **all the features across all output time steps**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFsDAwVt4_rq"
   },
   "source": [
    "For the multi-step model, the training data again consists of hourly samples. However, here, the models will learn to predict 24 hours into the future, given 24 hours of the past.\n",
    "\n",
    "Here is a `Window` object that generates these slices from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cFYtsz6XiGw"
   },
   "outputs": [],
   "source": [
    "OUT_STEPS = 24\n",
    "multi_window = WindowGenerator(input_width=24,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS)\n",
    "\n",
    "multi_window.plot()\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lg8SInh9Jzd"
   },
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axwpoWYOApJL"
   },
   "source": [
    "A simple baseline for this task is to repeat the last input time step for the required number of output time steps:\n",
    "\n",
    "![Repeat the last input, for each output step](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_last.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5iaHSaJ9Rxv"
   },
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])\n",
    "\n",
    "last_baseline = MultiStepLastBaseline()\n",
    "last_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Last'] = last_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(last_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvHZ93ObAfMA"
   },
   "source": [
    "Since this task is to predict 24 hours into the future, given 24 hours of the past, another simple approach is to repeat the previous day, assuming tomorrow will be similar:\n",
    "\n",
    "![Repeat the previous day](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_repeat.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8Y1uMhGwIRs"
   },
   "outputs": [],
   "source": [
    "class RepeatBaseline(tf.keras.Model):\n",
    "  def call(self, inputs):\n",
    "    return inputs\n",
    "\n",
    "repeat_baseline = RepeatBaseline()\n",
    "repeat_baseline.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)\n",
    "multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(repeat_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbndS-ct9C2Q"
   },
   "source": [
    "### Single-shot models\n",
    "\n",
    "One high-level approach to this problem is to use a \"single-shot\" model, where the model makes the entire sequence prediction in a single step.\n",
    "\n",
    "This can be implemented efficiently as a `tf.keras.layers.Dense` with `OUT_STEPS*features` output units. The model just needs to reshape that output to the required `(OUTPUT_STEPS, features)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NCKS4m1VKrDQ"
   },
   "source": [
    "#### Linear\n",
    "\n",
    "A simple linear model based on the last input time step does better than either baseline, but is underpowered. The model needs to predict `OUTPUT_STEPS` time steps, from a single input time step with a linear projection. It can only capture a low-dimensional slice of the behavior, likely based mainly on the time of day and time of year.\n",
    "\n",
    "![Predict all timesteps from the last time-step](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_dense.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfRz_WVhIQcd"
   },
   "outputs": [],
   "source": [
    "multi_linear_model = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\n",
    "    # Shape => [batch, 1, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_linear_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)\n",
    "multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(multi_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi2TMHk2IRrh"
   },
   "source": [
    "#### Dense\n",
    "\n",
    "Adding a `tf.keras.layers.Dense` between the input and output gives the linear model more power, but is still only based on a single input time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icsBAjCzMaMl"
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34lCZrWYNBwd"
   },
   "source": [
    "A convolutional model makes predictions based on a fixed-width history, which may lead to better performance than the dense model since it can see how things are changing over time:\n",
    "\n",
    "![A convolutional model sees how things change over time](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_conv.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weBjeZAFJOP4"
   },
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8022xOKxOO92"
   },
   "source": [
    "A recurrent model can learn to use a long history of inputs, if it's relevant to the predictions the model is making. Here the model will accumulate internal state for 24 hours, before making a single prediction for the next 24 hours.\n",
    "\n",
    "In this single-shot format, the LSTM only needs to produce an output at the last time step, so set `return_sequences=False` in `tf.keras.layers.LSTM`.\n",
    "\n",
    "![The LSTM accumulates state over the input window, and makes a single prediction for the next 24 hours](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_lstm.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bf1ks6RTzF64"
   },
   "outputs": [],
   "source": [
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units].\n",
    "    # Adding more `lstm_units` just overfits more quickly.\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features].\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features].\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])\n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(multi_lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5n-1cDW12Vo"
   },
   "source": [
    "### Advanced: Autoregressive model\n",
    "\n",
    "The above models all predict the entire output sequence in a single step.\n",
    "\n",
    "In some cases it may be helpful for the model to decompose this prediction into individual time steps. Then, each model's output can be fed back into itself at each step and predictions can be made conditioned on the previous one, like in the classic <a href=\"https://arxiv.org/abs/1308.0850\" class=\"external\">Generating Sequences With Recurrent Neural Networks</a>.\n",
    "\n",
    "One clear advantage to this style of model is that it can be set up to produce output with a varying length.\n",
    "\n",
    "You could take any of the single-step multi-output models trained in the first half of this tutorial and run in an autoregressive feedback loop, but here you'll focus on building a model that's been explicitly trained to do that.\n",
    "\n",
    "![Feedback a model's output to its input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/multistep_autoregressive.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKRreBbULRXY"
   },
   "source": [
    "#### RNN\n",
    "\n",
    "This tutorial only builds an autoregressive RNN model, but this pattern could be applied to any model that was designed to output a single time step.\n",
    "\n",
    "The model will have the same basic form as the single-step LSTM models from earlier: a `tf.keras.layers.LSTM` layer followed by a `tf.keras.layers.Dense` layer that converts the `LSTM` layer's outputs to model predictions.\n",
    "\n",
    "A `tf.keras.layers.LSTM` is a `tf.keras.layers.LSTMCell` wrapped in the higher level `tf.keras.layers.RNN` that manages the state and sequence results for you (Check out the [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn) guide for details).\n",
    "\n",
    "In this case, the model has to manually manage the inputs for each step, so it uses `tf.keras.layers.LSTMCell` directly for the lower level, single time step interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5tz3Nu0R5JG"
   },
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OXVM9G1U7xR"
   },
   "outputs": [],
   "source": [
    "feedback_model = FeedBack(units=32, out_steps=OUT_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph5uFSfTUNho"
   },
   "source": [
    "The first method this model needs is a `warmup` method to initialize its internal state based on the inputs. Once trained, this state will capture the relevant parts of the input history. This is equivalent to the single-step `LSTM` model from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM2K_LLdRjDZ"
   },
   "outputs": [],
   "source": [
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JkaSYaZ9eB7"
   },
   "source": [
    "This method returns a single time-step prediction and the internal state of the `LSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9Fz6NTKXXwU"
   },
   "outputs": [],
   "source": [
    "prediction, state = feedback_model.warmup(multi_window.example[0])\n",
    "prediction.shape\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "toc_visible": true,
   "provenance": [],
   "collapsed_sections": [
    "vtmu2IBPgPG8",
    "FYyEaqiD6j4s",
    "_8im1ttOWlRB",
    "tFZukGXrJoGo",
    "xCvD-UaUzYMw",
    "D1bbPiR3VAm_",
    "E4aOJScj52Yu",
    "W18e6da1cNbw",
    "j5dv_whJdswH",
    "CrpU6gwSJome",
    "H4crpOcoMlSe",
    "pYglOCKehi8F",
    "b5rUJ_2YMWzG",
    "9k7S5IHNhSNF",
    "dfbCrf5q3P6n",
    "dsc9pur_mHsx",
    "UwhY2f_Nn0_K",
    "I42Er9Du6co1",
    "5lg8SInh9Jzd",
    "tbndS-ct9C2Q",
    "NCKS4m1VKrDQ",
    "zi2TMHk2IRrh",
    "icsBAjCzMaMl",
    "weBjeZAFJOP4",
    "d5n-1cDW12Vo",
    "PKRreBbULRXY",
    "hGjcJsAQJUkI"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
